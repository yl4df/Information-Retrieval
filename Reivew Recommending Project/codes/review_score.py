# -*- coding: utf-8 -*-
"""IR

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VxsczWErhccMBnMhp3wSOrydSbRZy2yb
"""

import en_core_web_sm
import numpy
import pandas as pd
import os
import csv
from sklearn.feature_extraction import DictVectorizer

# change as appropriate
directory = os.getcwd()+'/drive/My Drive/IR Project/data'

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import nltk
nltk.download('wordnet')
stemmer = SnowballStemmer("english")

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

from google.colab import drive
drive.mount('/content/drive')

reviewlist = []
for file in os.listdir(directory):
    if file.endswith("Video_Games.csv"):
        with open(directory + "/" + file, newline='') as file:
            csvfile = csv.reader(file)
            for row in csvfile:
              review = row[5] ## reviewText is the 6th column
              reviewlist.append(review)

processed_reviews = []
for review in reviewlist:
    processed_reviews.append(preprocess(review))
final_reviews = []
for i in processed_reviews:
  if len(i)==0:
      final_reviews.append(" ") 
  else:
    final_reviews.append(" ".join(i))

"""To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut "Command/Ctrl+Enter". To edit the code, just click the cell and start editing.

Variables that you define in one cell can later be used in other cells:
"""

topic_keywords0 = [
"gardus sorri wait husband serious musti tell bunch sit tear",
"conveni project catcher littl lose felt wouldn nest contrapt space",
"open mask leaf resist ensur littl filter unfortun definit unit",
"instruct climb attic buck save duct push unscrew feet marvel",
"thank element great project conveni catcher speed lose clutch gardus",
"great good washer tube lazi clod foot angl figur plier",
"price better great littl project conveni catcher month quick problem",
"cycl long auto good run potenti gallon bucket jean adapt",
"speed clutch perfect high fast water replac pipe half slow",
"line glad wad get help buy free shop tri necessari",

]
topic_keywords1 = [
"bulb filter light money better stock replac work brighter mirror",
"perfect love awesom need work fine exact replac match origin",
"bright light super instal nice door easi switch look wire",
"instal bolt tool work need come tire mount wire like",
"good nice qualiti product fit price work look like great",
"great work easi product instal look price happi right issu",
"thank advertis cool work honda brake leak dodg motorcycl upgrad",
"excel describ expect fast ship item product great valu arriv",
"problem batteri jeep work month return instal helmet toyota perfect",
"look sticker cover nice great hold bike paint good clean"

]

topic_keywords2 = [
"thank soap expect love provenc time shampoo receiv perfect compliment",
"favorit love expens price gift amazon look product razor continu",
"nice fragranc scent soap skin smell light purchas wonder hand",
"excel product year lavend best love great hair store happi",
"scent cream hair bodi product nice shower skin like brush",
"toothpast skin soap size feel pleas favorit leav regular soft",
"soap shampoo best great smell hair awesom clean pleasant gentl",
"hair work like shampoo great shave condition look smell good",
"love great product stuff smell make amaz price scent want",
"good product hair recommend work shampoo polish color nail time",

]

topic_keywords3 = [
"thank download album purchas happi amazon great listen easi buy",
"classic cool enjoy perfect rock christma group song music smooth",
"love like song best music expect item describ star daughter",
"song like album music love track sound great listen hear",
"favorit beauti memori voic song time bring okay love countri",
"nice product song arriv varieti brown cover funni interest music",
"good song music qualiti sound price free pretti clear wife",
"version worship relax song work love music prais jonathan christian",
"great song awesom music band artist dope advertis cute princ",
"excel tune beat danc great wonder love song catchi oldi",
]

topic_keywords4 = [
"play like time stori good charact great graphic enjoy love",
"fallout evil resid horror hunt mechan dumb exel slash penni",
"fighter deliv remast phone arcad persona saturn street dont emblem",
"love daughter cute mario super pokemon nintendo wife mega refund",
"great cool alright duti madden fifa microsoft wrestl telltal cheaper",
"good awesom love like play gift kid best enjoy great",
"song husband lego sega genesi sale sonic season dreamcast danc",
"thank excel work product great perfect nice describ condit fast",
"work mous control xbox case need deliveri great download window",
"grandson control headset keyboard work button batteri sound comfort grip",
]




topic_keywords = topic_keywords4

weight0 = np.array([
[0.023,0.018,0.017,0.016,0.016,0.016,0.016,0.016,0.016,0.016],
[0.054,0.054,0.054,0.039,0.039,0.028,0.028,0.027,0.027,0.027],
[0.020,0.016,0.016,0.015,0.015,0.015,0.014,0.012,0.012,0.011],
[0.020,0.020,0.020,0.020,0.015,0.015,0.014,0.013,0.013,0.013],
[0.085,0.043,0.019,0.006,0.006,0.006,0.005,0.005,0.005,0.005],
[0.093,0.053,0.045,0.040,0.034,0.023,0.019,0.016,0.016,0.016],
[0.209,0.023,0.019,0.006,0.005,0.005,0.005,0.004,0.004,0.004],
[0.073,0.058,0.049,0.038,0.027,0.025,0.025,0.024,0.024,0.024],
[0.026,0.024,0.023,0.022,0.015,0.014,0.013,0.013,0.013,0.011],
[0.076,0.030,0.030,0.023,0.022,0.019,0.018,0.017,0.016,0.016]])

weight1 = np.array([
[0.025,0.018,0.016,0.014,0.011,0.010,0.009,0.008,0.008,0.008],
[0.164,0.060,0.050,0.036,0.035,0.034,0.032,0.027,0.018,0.017],
[0.053,0.039,0.013,0.013,0.012,0.011,0.011,0.011,0.010,0.010],
[0.009,0.008,0.006,0.006,0.006,0.005,0.005,0.005,0.005,0.005],
[0.238,0.083,0.072,0.050,0.047,0.046,0.034,0.034,0.029,0.024],
[0.290,0.195,0.072,0.070,0.059,0.044,0.041,0.015,0.014,0.013],
[0.048,0.045,0.015,0.014,0.012,0.011,0.011,0.010,0.009,0.008],
[0.069,0.060,0.052,0.049,0.044,0.035,0.032,0.025,0.020,0.020],
[0.013,0.010,0.009,0.009,0.009,0.009,0.008,0.007,0.007,0.007],
[0.011,0.009,0.008,0.007,0.007,0.007,0.007,0.007,0.007,0.007],
])

weight2 = np.array([
 [0.036,0.032,0.026,0.024,0.019,0.017,0.015,0.015,0.014,0.014],
 [0.039,0.021,0.018,0.016,0.014,0.014,0.013,0.011,0.011,0.011],
 [0.033,0.023,0.021,0.016,0.014,0.014,0.013,0.013,0.012,0.012],
 [0.040,0.032,0.023,0.023,0.019,0.019,0.018,0.017,0.017,0.017],
 [0.013,0.011,0.011,0.010,0.009,0.009,0.008,0.008,0.008,0.007],
 [0.020,0.020,0.018,0.013,0.012,0.012,0.012,0.012,0.011,0.010],
 [0.026,0.024,0.023,0.022,0.020,0.019,0.018,0.018,0.014,0.014],
 [0.028,0.025,0.021,0.018,0.016,0.014,0.013,0.013,0.010,0.010],
 [0.084,0.069,0.038,0.034,0.029,0.018,0.017,0.016,0.016,0.015],
 [0.041,0.017,0.015,0.013,0.012,0.010,0.010,0.010,0.009,0.009],
])


weight3 = np.array([
[0.061,0.042,0.037,0.031,0.026,0.025,0.023,0.020,0.020,0.019],
[0.087,0.048,0.044,0.042,0.029,0.025,0.017,0.016,0.015,0.014],
[0.536,0.106,0.095,0.028,0.028,0.023,0.019,0.015,0.011,0.010],
[0.010,0.009,0.009,0.008,0.008,0.007,0.006,0.006,0.006,0.006],
[0.050,0.049,0.029,0.027,0.026,0.024,0.021,0.021,0.020,0.020],
[0.175,0.056,0.015,0.012,0.010,0.010,0.009,0.009,0.009,0.009],
[0.506,0.073,0.055,0.052,0.047,0.026,0.025,0.018,0.009,0.009],
[0.021,0.016,0.015,0.013,0.012,0.011,0.011,0.011,0.010,0.010],
[0.508,0.174,0.124,0.033,0.011,0.011,0.008,0.008,0.006,0.006],
[0.086,0.044,0.030,0.030,0.027,0.026,0.023,0.023,0.018,0.018],
])

weight4 = np.array([
[0.008,0.007,0.005,0.005,0.005,0.005,0.004,0.004,0.004,0.004],
[0.012,0.010,0.009,0.008,0.008,0.007,0.007,0.006,0.006,0.006],
[0.015,0.014,0.013,0.011,0.009,0.008,0.007,0.007,0.007,0.007],
[0.342,0.050,0.030,0.020,0.019,0.018,0.011,0.008,0.008,0.007],
[0.458,0.057,0.011,0.010,0.009,0.008,0.007,0.007,0.007,0.006],
[0.158,0.049,0.031,0.031,0.031,0.031,0.030,0.020,0.020,0.018],
[0.018,0.018,0.015,0.015,0.014,0.013,0.013,0.013,0.012,0.012],
[0.085,0.081,0.072,0.068,0.053,0.040,0.034,0.029,0.029,0.027],
[0.023,0.015,0.015,0.013,0.013,0.011,0.010,0.010,0.009,0.009],
[0.013,0.011,0.011,0.008,0.008,0.007,0.007,0.007,0.006,0.006],
])


weight = weight4

words = []
for i in topic_keywords:
  words.append(i.split())
flat_list = [item for sublist in words for item in sublist]
similarity = []

def getIndexPositions(listOfElements, element):
    ''' Returns the indexes of all occurrences of give element in
    the list- listOfElements '''
    indexPosList = []
    indexPos = 0
    while True:
        try:
            # Search for item in list from indexPos to the end of list
            indexPos = listOfElements.index(element, indexPos)
            # Add the index position in list
            indexPosList.append(indexPos)
            indexPos += 1
        except ValueError as e:
            break
 
    return indexPosList

for review in final_reviews[1:]:
  score = [0]*10
  for word in review.split():
    if word in flat_list:
      indices = getIndexPositions(flat_list, word)
      for index in indices:
        topic = index//10
        position = index%10
        score[topic] += weight[topic][position]
  similarity.append(score)

df = pd.read_csv("/content/drive/My Drive/IR Project/data/Video_Games.csv",low_memory=False)
df2 = pd.DataFrame(similarity)
output = pd.concat([df, df2], axis=1)
output.to_csv("/content/drive/My Drive/IR Project/data with scores/Video_Games with score.csv",index = False, header=True)

df3 = pd.read_csv("/content/drive/My Drive/IR Project/data with scores/Video_Games with score.csv",low_memory=False)
print(df3[10:20])
print(len(df3))

counter = 0
for i in similarity:
  if sum(i)==0:
    counter+=1
print(counter)

import spacy

nlp = en_core_web_sm.load()
topic_labels = [i for i in range(10)
]

import itertools
import numpy as np
# Use pipe to run this in parallel
topic_docs = list(nlp.pipe(topic_keywords,
  batch_size=100000,
  n_threads=100))
topic_vectors = np.array([doc.vector
  if doc.has_vector else spacy.vocab[0].vector
  for doc in topic_docs])

keywords = final_reviews[1:]

topic_vectors=weight.dot(topic_vectors)

keyword_docs = list(nlp.pipe(keywords,
  batch_size=100000,
  n_threads=100))
keyword_vectors = np.array([doc.vector
  if doc.has_vector else spacy.vocab[0].vector
  for doc in keyword_docs])
from sklearn.metrics.pairwise import cosine_similarity
# use numpy and scikit-learn vectorized implementations for performance
simple_sim = cosine_similarity(keyword_vectors, topic_vectors)
df = pd.read_csv("/content/drive/My Drive/IR Project/data/Video_Games.csv",low_memory=False)
df2 = pd.DataFrame(simple_sim)
output = pd.concat([df, df2], axis=1)
output.to_csv("/content/drive/My Drive/IR Project/data with scores/Game with score.csv",index = False, header=True)
df3 = pd.read_csv("/content/drive/My Drive/IR Project/data with scores/Game with score.csv",low_memory=False)
print(df3[10:20])